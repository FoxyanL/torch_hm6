# Отчёт по домашнему заданию: Генератор текста на базе Transformer

## Задание: Построение авторегрессивного генератора текста

Разработан генератор текста на основе архитектуры Transformer, использующий только декодерную часть. Модель обучена на корпусе текстов (классическая русская литература) с целью авторегрессивной генерации текста — предсказания следующего токена на основе контекста.

## 1. Архитектура модели

Создан класс `GeneratorTransformer`, реализующий Transformer-декодер с механизмом внимания. Входные данные подаются с позиционной эмбеддингой и маскированием будущих токенов для корректной авторегрессии. Модель обучалась с использованием CrossEntropyLoss и оптимизатора Adam.

## 2. Токенизация

Для токенизации использован токенизатор Byte-Pair Encoding (BPE), построенный с помощью библиотеки `tokenizers`. Использовался предобученный токенизатор из урока. Это позволило эффективно разбивать текст на токены и корректно восстанавливать текст из токенов.

## 3. Обучение модели

- Использовались параметры: batch_size=1, max_length=128 токенов, learning_rate=1e-4.
- Обучение прошло за 3 эпохи с постепенным снижением среднего значения loss с ~4.7 до ~3.67.
- Для подготовки данных применялся скользящий "окно"-подход с максимальной длиной контекста и смещением.
- Важной частью была обработка специальных токенов начала и конца последовательности (BOS и EOS).

| Эпоха  | Средняя потеря (Avg loss) |
|--------|---------------------------|
| Epoch 1| 4.7188                    |
| Epoch 2| 3.9925                    |
| Epoch 3| 3.6723                    |

## 4. Авторегрессивная генерация текста

Реализован метод `generate` с учётом сдвига контекста — при каждом шаге входной контекст сдвигается на один токен влево, добавляя предсказанный токен в конец. Это позволяет последовательно генерировать текст до достижения максимальной длины или EOS.

*Пример генерации без beam search:*


Вы: проговорил он

Бот: проговорил он, – воскликнул Алеша.
– А этот, я вдруг сказал, вы так-то мне нет, чтобы тебе! – сказал он. – крикнул он, пожалуй куда, – сказал-то и зачем, – и сказал, может быть, скажете, – показал его


## 5. Beam search (дополнительная задача)

Для повышения качества генерации реализован beam search с несколькими лучшими гипотезами. Он позволяет выбирать более осмысленные варианты ответа, но повышается вероятность повторений.

*Пример генерации с beam search:*  

Вы: Боже тебя сохрани

Бот: Боже тебя сохранить?
– Да, – проговорил Алеша.
– Ну, – спросил Алеша.
– Ну, – проговорил он, – проговорил он, – проговорил он.
– Да, – проговорил он, – крикнул Алеша.


## 6. Тестирование и интерфейс

Создан простой консольный интерфейс для общения с моделью. Ввод пользователя обрабатывается токенизатором, затем модель генерирует ответ, который выводится в читаемом виде. Предусмотрена возможность выхода по команде `quit`.

# Выводы

- Реализованный Transformer-декодер успешно обучен и способен генерировать связный текст на русском языке.
- Авторегрессивная генерация с правильным сдвигом контекста обеспечивает корректность предсказаний.
- Beam search неплохо улучшает качество генерируемого текста.
- Применение токенизатора из урока и обучения на классических текстах позволяет получить адекватные ответы, хотя модель ещё требует доработки для уменьшения повторов и повышения смысловой связности.
- Разработка интерфейса позволяет быстро оценить работу модели и использовать её для диалогов.